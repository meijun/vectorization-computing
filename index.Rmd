---
title: "Vectorized Computing"
author: "Jun Mei"
date: "November 29, 2017"
output: 
  ioslides_presentation:
    transition: faster
---

## Contents

- Why?
- What & How?
- Experiments on performance

## MIMD or SIMD?

|  Technique  |  Pursuing  |  Blocked by  |
| :---------: | :--------: | :----------: |
|    SISD     | Higher speed (GHz) | Physics |
|    MIMD     | More CPU cores | Synchronization |
|    SIMD     | More CUDA cores | Price :) |

## CPU or GPU?

|  Technique  |  Instruction  |  Vector length  |
| :---------: | :-----------: | :-------------: |
|   CPU SIMD  | SSE4.1 SSE4.2 AVX AVX2 FMA | Commonly <= 256 bit |
|   GPU SIMD  | N/A, use CUDA | Thousands of Bytes |

## Programming

|  Technique  |  Hard mode  |  Easy mode  |
| :---------: | :---------: | :---------: |
|   CPU SIMD  | C/C++, assembly; instruction-orientated | NumPy |
|   CPU MIMD  | Java, etc; dead lock, data race | Java actor, Go, Rust |
|   GPU SIMD  | CUDA; register-orientated | ??? |

## GPU SIMD easy mode? | Is there a GPU backend for Numpy? Money is no issue

On [Reddit](https://www.reddit.com/r/Python/comments/1mw9mb/is_there_a_gpu_backend_for_numpyscipy_money_is_no/), four years ago.

- PyCUDA and PyOpenCL: Register-orientated :(
- NumbaPro: Register-orientated :(
- Theano: Died now :(
- gnumpy: Only a small subset of NumPy; Died now :(

> - Currently, PyTorch is free for everyone.
> - PyTorch feature: Tensor computation (like NumPy) with strong GPU acceleration.

## Distributed programming

- Target: balance on Consistency, Availability and Partition tolerance

> - MapReduce, by Google, in 2004
> - Hadoop (fair mode), Spark (easy mode)
> - MPI (hard mode)

## Matrix multiplication | A = BxC

```{python eval=FALSE}
for i in range(m):
  for j in range(n):
    for k in range(r):
      A[i][j] += B[i][k] * C[k][j]
```

## Matrix multiplication | Vectorized

```{python eval=FALSE}
for i in range(m):
  for j in range(n):
    A[i, j] = np.sum(B[i, :] * C[:, j])
```

```{python eval=FALSE}
for i in range(m):
  for j in range(n):
    for k in range(r):
      A[i][j] += B[i][k] * C[k][j]
```

## Matrix multiplication | Fully vectorized

```{python eval=FALSE}
m_r_n = B.reshape(m, r, 1) * C.reshape(1, r, n)
A = np.sum(m_r_n, axis=1)
```

```{python eval=FALSE}
for i in range(m):
  for j in range(n):
    for k in range(r):
      A[i][j] += B[i][k] * C[k][j]
```

## Boardcasting

```{python}
import numpy as np
B = np.array([[0, 1]])  # shape: 1x2
C = np.array([[0],
              [1],
              [2]])  # shape: 3x1
A = B + C
print(A)
```

## Floyd-Warshall algorithm

G is an nxn adjacency matrix

```{python eval=FALSE}
for k in range(n):
  for i in range(n):
    for j in range(n):
      G[i][j] = min(G[i][j], G[i][k] + G[k][j])
```

## Floyd-Warshall algorithm | Vectorized

```{python eval=FALSE}
for k in range(n):
  n_n = G[:, k].reshape(n, 1) + G[k, :].reshape(1, n)
  G = np.minimum(G, n_n)
```

```{python eval=FALSE}
for k in range(n):
  for i in range(n):
    for j in range(n):
      G[i][j] = min(G[i][j], G[i][k] + G[k][j])
```

## Experiments

```{python eval=FALSE}
n = int(sys.argv[1])

g = [[random.random() for _ in range(n)] for _ in range(n)]
floyd(g, n)  # warm up

g = [[random.random() for _ in range(n)] for _ in range(n)]
tic = time.time()
floyd(g, n)
toc = time.time()

print(toc - tic)
```

```{r echo=FALSE}
library(ggplot2)
library(scales)
f0 <- read.csv("src/floyd/summary.csv")
m0 <- read.csv("src/matmul/summary.csv")
lp <- function(df) {
	ggplot(df, aes(x=N, y=Time, col=Method)) +
		geom_line() +
		geom_point() +
		scale_x_continuous(trans=log2_trans()) +
		scale_y_log10()
}
```

## Floyd-Warshall algorithm

```{r echo=FALSE}
lp(f0)
```

## for | for_python << for_go < for_java < for_c

```{r echo=FALSE}
f1 <- f0[f0$Method %in% c("for_c", "for_go", "for_java", "for_python"),]
lp(f1)
```

## k_cpu | k_numpy = k_tf = k_torch = for_java < for_c

```{r echo=FALSE}
f1 <- f0[f0$Method %in% c("for_c", "for_java", "k_numpy", "k_torch", "k_tf"),]
lp(f1)
```

## k_gpu | for_java < for_c < k_tf_gpu = k_torch_gpu

```{r echo=FALSE}
f1 <- f0[f0$Method %in% c("for_c", "for_java", "k_torch_gpu", "k_tf_gpu"),]
lp(f1)
```

## Matrix multiplication

```{r echo=FALSE, fig.height=5.5}
lp(m0)
```

## for | for_python << for_go < for_java <= for_c

```{r echo=FALSE}
m1 <- m0[m0$Method %in% c("for_c", "for_go", "for_java", "for_python"),]
lp(m1)
```

## ij | ij_*_gpu < ij_cpu < for\_+

```{r echo=FALSE}
m1 <- m0[m0$Method %in% c("for_c", "for_java", "ij_numpy", "ij_tf", "ij_torch", "ij_tf_gpu", "ij_torch_gpu"),]
lp(m1)
```

## full | full_cpu < for_+ < full_tf_gpu < full_torch_gpu

```{r echo=FALSE}
m1 <- m0[m0$Method %in% c("for_c", "for_java", "full_numpy", "full_tf", "full_torch", "full_tf_gpu", "full_torch_gpu"),]
lp(m1)
```

## mat | for_+ < full_\*\_gpu < mat_\*

```{r echo=FALSE}
m1 <- m0[m0$Method %in% c("for_c", "for_java", "full_tf_gpu", "full_torch_gpu", "mat_numpy", "mat_tf", "mat_torch", "mat_tf_gpu", "mat_torch_gpu"),]
lp(m1)
```

## mat only | tf < numpy < torch < tf_gpu < torch_gpu (mat_)

```{r echo=FALSE}
m1 <- m0[m0$Method %in% c("mat_numpy", "mat_tf", "mat_torch", "mat_tf_gpu", "mat_torch_gpu"),]
lp(m1)
```

## Issue #1408 Â· tensorflow/tensorflow | Generalized matrix multiplication with semiring?

> Closing since I think this is out of reach of easy contributions. A trivial implementation is trivial, but users are likely to want fast versions that are hard to write. -- Geoffrey Irving at @openai

https://github.com/tensorflow/tensorflow/issues/1408

## Summary | Floyd-Warshall algorithm

- for | for_python << for_go < for_java < for_c
- k_cpu | k_numpy = k_tf = k_torch = for_java < for_c
- k_gpu | for_java < for_c < k_tf_gpu = k_torch_gpu

> - I.e.
> - for_python << for_go < k_numpy = k_tf = k_torch = for_java < for_c < k_tf_gpu = k_torch_gpu

> - I.e.
> - for_python << for_go < k_cpu = for_java < for_c < k_gpu

## Summary | Matrix multiplication

- for | for_python << for_go < for_java <= for_c
- ij | ij_*_gpu < ij_cpu < for\_+
- full | full_cpu < for_+ < full_tf_gpu < full_torch_gpu
- mat | for_+ < full_\*\_gpu < mat_\*

> - I.e.
> - ij_gpu < ij_cpu < full_cpu < for_java <= for_c < full_tf_gpu < full_torch_gpu < mat_cpu < mat_gpu

> - I.e.
> - ij < full_cpu < for_java/c < full_gpu < mat_\*

## Conclusion

- for_python << for_go < k_cpu = for_java < for_c < k_gpu
- ij < full_cpu < for_java/c < full_gpu < mat_\*

> - ij hurts
> - Fully-vectorized gpu code, or java/c

> - The code is available
> - at https://github.com/meijun/vectorized-computing
